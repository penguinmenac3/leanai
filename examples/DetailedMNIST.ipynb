{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88fe4c0-f7e5-4c78-a967-f97f60df1e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305b8f65-8354-4254-b475-ad9179fb0d24",
   "metadata": {},
   "source": [
    "# Example: Detailed MNIST (with custom code)\n",
    "\n",
    "> This example is totally overkill for MNIST, but it explains how you would use leanai on your problem.\n",
    "\n",
    "In this example we will have a look at how to implement all the details of MNIST.\n",
    "For MNIST, this will be totally overkill, but it is a nice simple example to teach all details.\n",
    "At the end of this tutorial, you should feel comfortable to implement your own task using MNIST without any trouble.\n",
    "\n",
    "> **Note**: This tutorial will assume, that you have already completed the Minimal MNIST tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f482afa8-2450-46fe-bb08-0e79c3c03b53",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "We will implement everything step by step in this notebook.\n",
    "Each section could be it's own python file or package if it gets more complicated, so keep that in mind.\n",
    "\n",
    "1. Dataset\n",
    "2. Model\n",
    "3. Training/Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db17cc0-49a8-4b02-8869-2f407659aa61",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First we will implement a `SimpleDataset`. The `SimpleDataset` provides us with samples to use for training and validation. Each sample is a tuple that has two entries, the first is the input to the model and the other is the output of the model. As these entries are often a general format related to the dataset, we will call them `DatasetInput` and `DatasetOutput`. In this case they are the same as the `NetworkInput` and `NetworkOutput`, but sometimes you will need transformers to convert the format.\n",
    "\n",
    "So with that explanation, let's define a DatasetInput and DatasetOutput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a19e04-e2ec-4ec7-a110-140038e49df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "MNISTInputType = namedtuple(\"MNISTInput\", [\"image\"])\n",
    "MNISTOutputType = namedtuple(\"MNISTOutput\", [\"class_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7dbc75-c4fd-4889-a710-1cd58480eaf6",
   "metadata": {},
   "source": [
    "Next we will implement our dataset by inheriting from `SimpleDataset`. This means we will only need to implement the constructor, where we set the sample tokens and getters for every field that was usesd in the Input and Output. Calling the getters and assembling the Types is automatically done by the parent class.\n",
    "\n",
    "**Sample Tokens** are a list of indentifiers (str/int) used to uniquely identify a sample in a dataset. The list that is set initially must contain all examples that are used during training.\n",
    "\n",
    "**Getters** are `get_image` and `get_class_id` in this example and return the respective data associated to the provided sample token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9af183-e65f-46f9-a75c-ff4f83c7dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from leanai.core.definitions import SPLIT_TRAIN\n",
    "from leanai.data.dataset import SimpleDataset\n",
    "\n",
    "\n",
    "class FashionMNISTDataset(SimpleDataset):\n",
    "    def __init__(self, split: str, data_path: str = \"\", download=True, shuffle=True) -> None:\n",
    "        super().__init__(\n",
    "            InputType=MNISTInputType,\n",
    "            OutputType=MNISTOutputType,\n",
    "        )\n",
    "        self.dataset = FashionMNIST(data_path, train=split == SPLIT_TRAIN, download=download)\n",
    "        # Provide a list with all sample_tokens\n",
    "        self.set_sample_tokens(range(len(self.dataset)))\n",
    "\n",
    "    def get_image(self, sample_token) -> Any:\n",
    "        image, _ = self.dataset[sample_token]\n",
    "        image = np.array(image, dtype=\"float32\")\n",
    "        return np.reshape(image, (28, 28, 1))\n",
    "\n",
    "    def get_class_id(self, sample_token) -> Any:\n",
    "        _, class_id = self.dataset[sample_token]\n",
    "        return np.array([class_id], dtype=\"uint8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dd392a-bbd5-4c35-828c-d96d0de71974",
   "metadata": {},
   "source": [
    "Let's see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc1b0d0-a68e-4fd6-b595-0919154df409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "(1,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuerst/.miniconda3/envs/ssf/lib/python3.8/site-packages/matplotlib/text.py:1223: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if s != self._text:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAVQUlEQVR4nO3df3DcZZ0H8Pd7N5sfTZO2aUsoJVJ+FKXCUTS2CIygKD86d1dQj5FxvHrDWO5OvdPzDx30Rm5u7oZzDhlG7ryLwgk3iqejjJVhFCgiIooNpdLSWsuPQlvaNKW0SdP82M1+7o9sNZQ8n2/Y72Z34Xm/ZjrZ7Gef7JPdvPvd3ef7PA/NDCLy5pepdQdEpDoUdpFIKOwikVDYRSKhsItEQmEXiYTCLhIJhT1yJI3kEMl/mebtryN5pNTujJnun1SOwi4AcK6ZffHYNyT/jOSWUqgfI7nsWM3Mbjez2bXppqShsMurkFwK4NsA/hrAXAA/BrCOZEMt+yXpKexyvMsB/MLMHjWzAoB/A7AYwMW17ZakpbDLVHjcZQI4u0Z9kQpR2OV4DwK4mOQlJBsB3ACgEcCs2nZL0lLY5VXM7HcA1gC4DcBeAAsAbAWwu5b9kvSoKa5xI2kAlprZM4H6XEwEvbv0H8G02kn90ZFdXoPkO0lmSS4E0ANg3eSgyxuTwi5TuRXAIQDbAbwC4BM17Y1UhMIuowCeIPnPx64ws4vMrM3MOszsejMbOlYj+VckD5XaFavfXSmX3rOLREJHdpFIKOwikajq+c6NbLJmtFbzLkWiMoIhjNkop6qlCjvJKzDxyW0WwDfN7Cbv9s1oxUpemuYuRcTxuK0P1sp+GU8yC+A/AFwJYBmAaydPhRSR+pLmPfsKAM+Y2XNmNgbguwBWV6ZbIlJpacK+GMCuSd/vLl33KiTXkuwl2ZvHaIq7E5E0ZvzTeDPrMbNuM+vOoWmm705EAtKEfQ+Arknfn1y6TkTqUJqwbwCwlOSppXnPHwGwrjLdEpFKK3vozcwKJD8F4KeYGHq7w8yerljPRKSiUo2zm9l9AO6rUF9EZAbpdFmRSCjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4lEVZeSlhrglKsK/1HKHYGy8zvc+iuXnxmstX/n16nuO+l3Y0MuWLP8WLr7TivpefGU+ZzpyC4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiobCLRELj7G9yzGbduhUKbj2z3N+rc9v1s/32w+FabmiF27ZhuOjWc/f3uvVUY+lJY/gJjyvoH0fT9I0NTmydp1NHdpFIKOwikVDYRSKhsItEQmEXiYTCLhIJhV0kEhpnf5Nzx2SRPM6+6/K5bv2j7/6FW/9l/2nB2gtNJ7ptrcUto+H973brZ/7nnmCtsPNF/4cnzBlPetySZOfNCxfHx9224wMD4aLT7VRhJ7kTwCCAcQAFM+tO8/NEZOZU4sj+XjM7UIGfIyIzSO/ZRSKRNuwG4H6ST5BcO9UNSK4l2UuyN4/RlHcnIuVK+zL+IjPbQ/IEAA+Q/J2ZPTL5BmbWA6AHANrZkW51QxEpW6oju5ntKX3dD+AeAP40JhGpmbLDTrKVZNuxywAuA7ClUh0TkcpK8zK+E8A9nJj32wDgO2b2k4r0SiqmODKSqv3YeUfc+ofn+HPKmzP5YO3nGX+++p6Hutz6+J/4fXvhq23BWvHJC9y287f4Y93tT+516wfes9it978z/I62M2E5/XkPPhus8WA40mWH3cyeA3Buue1FpLo09CYSCYVdJBIKu0gkFHaRSCjsIpGgpdyy9/VoZ4et5KVVu79oeMseJzy/R645361f+aWH3fpZzS+59cFic7A2ZulO4Lxt+8Vufei5OcFaZixhy+SE8ninvxS05f3j6LyN4d+9ZXWf25bfWBisPbX+Vhw5uGvK3uvILhIJhV0kEgq7SCQUdpFIKOwikVDYRSKhsItEQuPs9SBhe+BUEp7fs5/w/7//4Dx/CmuSrLO28ZA1um0Pjbemuu/+QniKaz5hjP+bO/wpsEecMXwAyBT85/QD730yWPtQxwa37VdOPydYe9zWY8AOapxdJGYKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEtmyuB1U81+F4O46c4NZfbp/t1vcV5rr1+dnwcs9tmWG37ZKcv19o/3h4HB0AsrnwUtVjlnXb/tPbf+zWR87KufUc/aWoL3DWAfiLrX/ptm3Fc249REd2kUgo7CKRUNhFIqGwi0RCYReJhMIuEgmFXSQSGmeP3MImf9vjZoa3XAaARhbc+kv5ecHajuG3um1/P+CfA3BF59NuPe+MpXvz7IHkcfKTcq+49RHzx+G9R/XCTn8cfZNbDUs8spO8g+R+klsmXddB8gGSO0pfw8+oiNSF6byM/xaAK4677gsA1pvZUgDrS9+LSB1LDLuZPQLg4HFXrwZwZ+nynQCuqmy3RKTSyn3P3mlme0uX9wHoDN2Q5FoAawGgGbPKvDsRSSv1p/E2sWJl8NMOM+sxs24z686hKe3diUiZyg17H8lFAFD6ur9yXRKRmVBu2NcBWFO6vAbAjyrTHRGZKYnv2UneDeASAAtI7gbwZQA3AfgeyesAvADgmpns5JtewrrxzPpzr60QHuvOzvNHRS+eu9mt94+3u/VD4/7nMHOzR4O1wUJ473YAODjs/+y3Ne116xuPLgnWFjb64+RevwFg59gCt760aZ9b/0pfeP+ErubjPw9/tcKl7wnW7PFfBWuJYTezawMl7fYg8gai02VFIqGwi0RCYReJhMIuEgmFXSQSmuJaDxKWkmaD/zR5Q2+7rjvLbfu+Wf6SyY+NLHbrCxsG3bo3zXRR02G3bVvniFtPGvbraAhP3x0cb3HbzsqMuvWk3/sdjf4y2J998B3BWtvZL7tt23POMdoZxdWRXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0RCYReJhMbZ6wBzjW69OOKPN3sWbB5z6wfG/SWP52b8qZ6NCUsue1sjX9DxvNu2P2EsfOPwqW69LRveEnphxh8n78r5Y92bR7rc+n1DZ7j16/70wWDt7p4PuG0bf/JYsEYLP186sotEQmEXiYTCLhIJhV0kEgq7SCQUdpFIKOwikXhjjbM7Sy6zwR8vZjbh/7WMXy+OOPObi/5YcxLL+2Phadz637e59V2FuW59X96vJy25PO5MsP718By3bXPG3y56YcOAWx8o+uP0nsGiv8y1N08fSO775+fvCNZ+ePj9btty6cguEgmFXSQSCrtIJBR2kUgo7CKRUNhFIqGwi0SirsbZ06yPnjRWbf6wZ00Nr17h1ndd5Y/jf/S83wRr+wptbtsnnW2NAWCOMyccAFoT1lcfsfD5Dy+N+dtJJ41Ve+vCA8AJzjj8uPnHuT15v29Jks4/2F1w1rT/c3+u/dy7yupS8pGd5B0k95PcMum6G0nuIbmp9G9VeXcvItUynZfx3wJwxRTX32Jmy0v/7qtst0Sk0hLDbmaPADhYhb6IyAxK8wHdp0g+VXqZH3yDQ3ItyV6SvXn47+9EZOaUG/avAzgdwHIAewHcHLqhmfWYWbeZdefQVObdiUhaZYXdzPrMbNzMigC+AcD/OFlEaq6ssJNcNOnbqwFsCd1WROpD4jg7ybsBXAJgAcndAL4M4BKSywEYgJ0Arq9EZ7xx9LQaFp3o1vOndrr1g2eF9wI/eqKzKTaA5au2ufWPd/6PW+8fb3frOTr7s+fnu23Pm7XTrT90eJlbP9Aw26174/QXtIbndAPAoaK///pJDa+49c8/8+FgrXOWP5b9zVP8Aaa8Fd369rz/lvVwMTwf/u+W/cxtew8WuvWQxLCb2bVTXH17WfcmIjWj02VFIqGwi0RCYReJhMIuEgmFXSQSdTXFdfTKd7n1E774XLC2vH2323ZZy6NufaToL0XtTbfcOrzYbXu06G/JvGPMHxY8XPCHoLIMDwPtH/OnuN78vL9s8foV/+XWv/TSVHOk/ijTYsHay+P+sN2HZvtLRQP+c3b9Wx4J1k5r3O+2vXdokVt/KWEKbGfusFtfkusP1j7Y9nu3bblDbzqyi0RCYReJhMIuEgmFXSQSCrtIJBR2kUgo7CKRqO44O/3lolf+6wa3+aVtTwdrR82fUpg0jp40buqZ0+AvGzya9x/m/Xl/CmuSM5v2BWtXt29y2z5y20q3ftHIp936s+/zp+euHw5P5ewv+L/3R55/n1vf+GKXWz9/yfPB2jlte9y2Sec2tGVH3Lo37RgAhorhv9dfj/jnH5RLR3aRSCjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBI0C883rrSWE7vs9I/9Q7De88mvue2/c/D8YK2r2d+O7pTGA259ftbf/tfTlvHHXN+a88dc7x062a0/fOhtbv2dbTuDtRz97Z4vmfWMW//4Zz/n1gvN/jLaA0vCx5NCq/+3137uy27902c85NYbnd/90Lg/jp70uCVtyZzEW4OgLeNvk33zqquDtV/t/BYOD++d8knRkV0kEgq7SCQUdpFIKOwikVDYRSKhsItEQmEXicR0tmzuAnAXgE5MbNHcY2a3kuwA8H8AlmBi2+ZrzMzdQzeTB2b1hccX7x1Y7vbltJbwWtsH8v766D89co5bP7nF3/7X23r4DGc+OQBsGpnr1n/S/3a3flKLv356X35OsPZyvtVte9SZVw0At9/yVbd+c5+/7vzVHRuDtXMb/XH0Q0X/WLQ1Yb39wWJzsDZi/voGhxPG4ducvwcAyJsfrayz5fPcjD+GP3BOeBvu8b7w/U7nyF4A8DkzWwbgfACfJLkMwBcArDezpQDWl74XkTqVGHYz22tmG0uXBwFsA7AYwGoAd5ZudieAq2aojyJSAa/rPTvJJQDOA/A4gE4z21sq7cPEy3wRqVPTDjvJ2QB+AOAzZvaqN5E2cYL9lCc6k1xLspdkb2F0KFVnRaR80wo7yRwmgv5tM/th6eo+kotK9UUAptwpz8x6zKzbzLobmvwPi0Rk5iSGnSQB3A5gm5lN/mh2HYA1pctrAPyo8t0TkUqZzlLSFwL4GIDNJDeVrrsBwE0AvkfyOgAvALgm6Qdlx4po2zUarBfNny750IHwVM/O5kG37fK2XW59+1F/GGfz8EnB2saGt7htW7Lh7Z4BYE6jP0W2tSH8mAHAglz4dz+1yd+a2JsGCgAbRvzf7W8WPuzWXyyEl+j+8dCZbtutR8OPOQDMS1jCe/NAuP3Rgr+N9ui4H42Rgj+UO6fJf07f1fFCsLYd/nbR/ec604Z/GW6XGHYzexRAKIWXJrUXkfqgM+hEIqGwi0RCYReJhMIuEgmFXSQSCrtIJKq7ZfORYWR+/mSw/P37L3Sb/+Pq7wdrP09Ybvneff646MCYP9Vz4azwqb7tzjg3AHTk/NOEk7Z8bk7Y/veVQvjMxNGMP5VzPDiqOmHfaHj6LAD8srjUreeL4S2bR50akHx+wsGxBW79pJbDwdpgITz9FQB2Dna49QOH/W2VR2b50Xp0/PRg7YoTw1uTA0DL/vBzlnH+VHRkF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiUdUtm9vZYStZ/qzYwx8Nb9l82t9ud9uumPu8W9844M/bftEZd80nLHmcy4SXDQaAWbkxt96cMN7cmA3PSc9MvVrYHxQTxtlbs37fkubatzeE53W3Zf053xlnW+PpyDq/+28OL0n1s9sSfu+C+X8T757zbLB2x/MXuG3nrApvs/24rceAHdSWzSIxU9hFIqGwi0RCYReJhMIuEgmFXSQSCrtIJKo/zp69LHyDor+GeRpDH1rp1lfesMGvt4XHRd/W2Oe2zcEfL25OGE9uzfhj4SPOc5j0v/mjw11ufTzhJzz0ylluPe+MN/cdbXfb5pzzB6bD24dguJCwZfOwP989m/FzM/KwP9d+/tbwuRNN9/l/ix6Ns4uIwi4SC4VdJBIKu0gkFHaRSCjsIpFQ2EUikTjOTrILwF0AOgEYgB4zu5XkjQA+AaC/dNMbzOw+72elnc9er/guf0364RNb3HrTy/7c6MFT/Pbtz4bXpc+M+mvOF3+7za3LG4s3zj6dTSIKAD5nZhtJtgF4guQDpdotZvbvleqoiMycxLCb2V4Ae0uXB0luA7B4pjsmIpX1ut6zk1wC4DwAj5eu+hTJp0jeQXJeoM1akr0ke/PwX66KyMyZdthJzgbwAwCfMbMBAF8HcDqA5Zg48t88VTsz6zGzbjPrzsHfT01EZs60wk4yh4mgf9vMfggAZtZnZuNmVgTwDQArZq6bIpJWYthJEsDtALaZ2VcnXb9o0s2uBrCl8t0TkUqZzqfxFwL4GIDNJDeVrrsBwLUkl2NiOG4ngOtnoH9vCLZhs1v3J0sma3+s/LbpFmOWN5PpfBr/KDDl4uLumLqI1BedQScSCYVdJBIKu0gkFHaRSCjsIpFQ2EUiobCLREJhF4mEwi4SCYVdJBIKu0gkFHaRSCjsIpFQ2EUiUdUtm0n2A3hh0lULAByoWgden3rtW732C1DfylXJvp1iZgunKlQ17K+5c7LXzLpr1gFHvfatXvsFqG/lqlbf9DJeJBIKu0gkah32nhrfv6de+1av/QLUt3JVpW81fc8uItVT6yO7iFSJwi4SiZqEneQVJLeTfIbkF2rRhxCSO0luJrmJZG+N+3IHyf0kt0y6roPkAyR3lL5Oucdejfp2I8k9pcduE8lVNepbF8mfkdxK8mmSf1+6vqaPndOvqjxuVX/PTjIL4PcAPgBgN4ANAK41s61V7UgAyZ0Aus2s5idgkHwPgCMA7jKzs0vXfQXAQTO7qfQf5Twz+3yd9O1GAEdqvY13abeiRZO3GQdwFYCPo4aPndOva1CFx60WR/YVAJ4xs+fMbAzAdwGsrkE/6p6ZPQLg4HFXrwZwZ+nynZj4Y6m6QN/qgpntNbONpcuDAI5tM17Tx87pV1XUIuyLAeya9P1u1Nd+7wbgfpJPkFxb685ModPM9pYu7wPQWcvOTCFxG+9qOm6b8bp57MrZ/jwtfUD3WheZ2TsAXAngk6WXq3XJJt6D1dPY6bS28a6WKbYZ/4NaPnblbn+eVi3CvgdA16TvTy5dVxfMbE/p634A96D+tqLuO7aDbunr/hr35w/qaRvvqbYZRx08drXc/rwWYd8AYCnJU0k2AvgIgHU16MdrkGwtfXACkq0ALkP9bUW9DsCa0uU1AH5Uw768Sr1s4x3aZhw1fuxqvv25mVX9H4BVmPhE/lkAX6xFHwL9Og3Ab0v/nq513wDcjYmXdXlMfLZxHYD5ANYD2AHgQQAdddS3/wWwGcBTmAjWohr17SJMvER/CsCm0r9VtX7snH5V5XHT6bIikdAHdCKRUNhFIqGwi0RCYReJhMIuEgmFXSQSCrtIJP4f2HCHruV6dsMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "debug_dataset = FashionMNISTDataset(\"train\", \"outputs\")\n",
    "feature, target = debug_dataset[0]\n",
    "print(feature.image.shape)\n",
    "print(target.class_id.shape)\n",
    "\n",
    "plt.imshow(feature.image[:, :, 0])\n",
    "plt.title(target.class_id)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6fad5f-57a2-495c-9e5b-1606761e99e1",
   "metadata": {},
   "source": [
    "Looks as expected, let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58625421-2fc5-4aa1-8d99-8aadf7bffc20",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now with a dataset implemented, let's implement our model.\n",
    "The model can be any pytorch model inheriting from (`torch.nn.Module`).\n",
    "\n",
    "However, you can also write models using layers from leanai, for a bit more keras like feeling.\n",
    "Let me show you how.\n",
    "\n",
    "Generally, we have to create our layers in the constructor and then implement the forward. That is it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9322c8f4-c592-4bd9-83c3-e9ee2c725ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from leanai.model.layers import Conv2D, Flatten, Dense, ImageConversion, Activation, Sequential, BatchNormalization, MaxPooling2D\n",
    "\n",
    "\n",
    "class ImageClassifierSimple(nn.Module):\n",
    "    def __init__(self, num_classes=10, logits=False):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            ImageConversion(standardize=False, to_channel_first=True),\n",
    "            Conv2D(kernel_size=(3, 3), filters=12),\n",
    "            Activation(\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(kernel_size=(3, 3), filters=18),\n",
    "            Activation(\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(kernel_size=(3, 3), filters=18),\n",
    "            Activation(\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            BatchNormalization(),\n",
    "            Conv2D(kernel_size=(3, 3), filters=18),\n",
    "            Activation(\"relu\"),\n",
    "            MaxPooling2D(),\n",
    "            BatchNormalization(),\n",
    "            Flatten(),\n",
    "            Dense(18),\n",
    "            Activation(\"relu\"),\n",
    "            Dense(num_classes),\n",
    "        ]\n",
    "        if not logits:\n",
    "            layers.append(Activation(\"softmax\", dim=1))\n",
    "        self.layers = Sequential(*layers)\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.layers(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da99ebc-f433-4802-b550-efc0eeba2cb3",
   "metadata": {},
   "source": [
    "# Training / Loss\n",
    "\n",
    "Next stop is implementing a custom loss, to tie everything together.\n",
    "\n",
    "For losses again any pytorch loss will work fine, but we will implement a leanai loss, as it comes with additional logging capabilities.\n",
    "Anywhere in your loss function you can simply call `self.log(name, scalar)` and it will add it to the tensorboard logging.\n",
    "\n",
    "Your loss is implemented again by implementing the `__init__` and the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cc9ce6e-5fab-4359-a56b-b897a4ecc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from leanai.core.experiment import Experiment\n",
    "from leanai.training.losses import SparseCrossEntropyLossFromLogits, Loss\n",
    "\n",
    "\n",
    "class MyLoss(Loss):\n",
    "    def __init__(self, parent: Experiment):\n",
    "        super().__init__(parent)\n",
    "        self.loss = SparseCrossEntropyLossFromLogits()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = self.loss(y_pred=y_pred, y_true=y_true)\n",
    "        self.log(\"loss/my_ce\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3419612-8b44-4f23-ad3d-9825262dc45b",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "As we have explained how this works, let's just use the experiment to run a training with this dataset, model and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c76959-f206-453c-8956-ac274de86210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/fuerst/.miniconda3/envs/ssf/lib/python3.8/site-packages/torch/_jit_internal.py:668: LightningDeprecationWarning: The `LightningModule.loaded_optimizer_states_dict` property is deprecated in v1.4 and will be removed in v1.6.\n",
      "  if hasattr(mod, name):\n",
      "\n",
      "  | Name  | Type                  | Params | In sizes       | Out sizes\n",
      "-----------------------------------------------------------------------------\n",
      "0 | model | ImageClassifierSimple | 8.6 K  | [2, 28, 28, 1] | [2, 10]  \n",
      "1 | loss  | MyLoss                | 0      | ?              | ?        \n",
      "-----------------------------------------------------------------------------\n",
      "8.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "8.6 K     Total params\n",
      "0.034     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d987f43f5304d04a6dac3bb442d64d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f72cb9b34cd249078256b99d522b6062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9f96e22b2da40e080d191ad5c2e08f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae092176075e4401acd6390c0f53178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da8f77e7b44756a9f8990a24496ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9331efa1e40f4e3c8ec488249d86be00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01927b921e314380ba13506036a1ab0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5436a9ddee784ab8af13a74a294b8f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c5c751e343489eb2212a70d888c8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862a47464e2149e499ef9ddef96a64f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a68ae321ea4a7eaee79253abbf2ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d56d1a7a71c4d4ab7f2f7e86b36086d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.optim import SGD\n",
    "from leanai.core.experiment import Experiment, set_seeds\n",
    "\n",
    "\n",
    "set_seeds()\n",
    "experiment = Experiment(\n",
    "    model=ImageClassifierSimple(num_classes=10, logits=True),\n",
    "    output_path=\"outputs\",\n",
    "    example_input=torch.zeros((2, 28, 28, 1), dtype=torch.float32),\n",
    ")\n",
    "experiment.run_training(\n",
    "    load_dataset=dict(\n",
    "        type=FashionMNISTDataset,\n",
    "        data_path=\"outputs\",\n",
    "    ),\n",
    "    build_loss=dict(\n",
    "        type=MyLoss\n",
    "    ),\n",
    "    build_optimizer=dict(\n",
    "        type=SGD,\n",
    "        lr=1e-3,\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd3f7a3-f910-47e2-af83-e18995b141f3",
   "metadata": {},
   "source": [
    "> **Note**: Unfortunately the progress bar does not work properly in jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfd6046-a3bd-4969-8ffd-96671e9c9d43",
   "metadata": {},
   "source": [
    "## Wrap-Up\n",
    "\n",
    "That is it for the tutorial. You might want to have a look at tensorboard though. Here you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "176567ac-8266-4960-9bad-63cb894674d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-629f6fbed82c07cd\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-629f6fbed82c07cd\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cc53ac-0887-4c5d-a052-4dc6bf11e78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SSF)",
   "language": "python",
   "name": "ssf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
